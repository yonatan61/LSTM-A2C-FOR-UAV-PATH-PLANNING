{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install pyglet\n",
        "!pip install imageio-ffmpeg"
      ],
      "metadata": {
        "id": "7y-HYlwVqbTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from collections import deque\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "from collections import defaultdict\n",
        "from moviepy.video.io.bindings import mplfig_to_npimage"
      ],
      "metadata": {
        "id": "B1frFiwbrq6V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class enviornment:\n",
        "  def __init__(self):\n",
        "    print('initialize env')\n",
        "    self.grid= np.zeros((21, 21))\n",
        "    self.total_user_distribution=self.users_generator()\n",
        "    self.state=self.reset() #for q1 \n",
        "    \n",
        "\n",
        "\n",
        "  def users_generator(self):\n",
        "    self.total_user_distribution=[]\n",
        "    for i in [[17,4,5,40],[12,6,4,15],[6,15,4,15],[15,15,5,20],[7,16,3,20]]:\n",
        "      \n",
        "      x,y,std,num=i[0],i[1],i[2],i[3]\n",
        "      X=np.abs(np.random.normal(x,std,num))  #by using abs all the users with -ve will be refelcted \n",
        "      Y=np.abs(np.random.normal(y,std,num))\n",
        "      user_distribution=[]\n",
        "\n",
        "      #refelect back the users during distribution  \n",
        "      for k, (i,j) in enumerate(zip(X,Y),):\n",
        "        if i >20:\n",
        "          X[k]=20-(i-20)\n",
        "        if j>20:\n",
        "          Y[k]=40-j \n",
        "\n",
        "      #ignore users thare are in the landing and no fly zone  \n",
        "      for i,j in zip(X,Y):\n",
        "        if i>=7 and i<= 9 and j>=18 :\n",
        "          continue\n",
        "        if i>=8 and i<= 14 and j>=10 and j<=13 :\n",
        "          continue\n",
        "        user_distribution.append((i,j))\n",
        "      self.total_user_distribution.append(user_distribution)\n",
        "    \n",
        "    return self.total_user_distribution\n",
        "  \n",
        "  #creat image\n",
        "  def render(self,Title=\"\",state=(0,0),optimal_path=[],path_plot=False):\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.rcParams[\"figure.figsize\"]=(10,9)\n",
        "    ax.imshow(self.grid, cmap='Greys',origin='lower')\n",
        "    rect = patches.Rectangle((7, 18), 2, 2, linewidth=2, edgecolor='black', facecolor='green')\n",
        "    ax.add_patch(rect) \n",
        "    rect = patches.Rectangle((8, 10), 6, 3, linewidth=2, edgecolor='black', facecolor='red')\n",
        "    ax.add_patch(rect) \n",
        "    plt.text(9.5, 11.2, \"No Fly Zone\", color='Black', fontsize=14)\n",
        "    plt.text(7.7, 18.7, \"SL\", color='Black', fontsize=14)\n",
        "    plt.xticks(np.arange(0, 21, 1),np.arange(0, 21, 1))\n",
        "    plt.yticks(np.arange(0, 21, 1),np.arange(0, 21, 1))\n",
        "    plt.title(Title)\n",
        "    plt.grid()\n",
        "\n",
        "\n",
        "    for t,symbol in zip(self.total_user_distribution,['x','o','*','#','+']):\n",
        "        for i in t:\n",
        "          plt.text(i[0], i[1], symbol, color='Black', fontsize=11)\n",
        "\n",
        "    rect = patches.Rectangle((state[0]-1.5, state[1]-1.5), 3, 3, linewidth=1, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    if path_plot:\n",
        "      for state, next_state in optimal_path:\n",
        "        plt.arrow(state[0],state[1],0.9*(next_state[0]-state[0]), 0.9*(next_state[1]-state[1]),head_width=0.1, color='Red',width=0.01)\n",
        "\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "    return fig\n",
        "  \n",
        "  \n",
        "\n",
        "  def is_outoff_grid(self,i, j):\n",
        "      if i < 0 or i > 20 or j < 0 or j > 20:\n",
        "          return True\n",
        "      elif i>=8 and i<= 14 and j>=10 and j<=13 :\n",
        "          return True\n",
        "      else:\n",
        "          return False\n",
        "\n",
        "  def surveillance(self,total_user_distribution,xnow,ynow,xnext,ynext,history):\n",
        "  \n",
        "    coverd_users_now=[]\n",
        "    coverd_users_next=[]\n",
        "    if (xnow,ynow) not in history:\n",
        "        \n",
        "        xmax=xnow+1.5\n",
        "        xmin=xnow-1.5\n",
        "        ymax=ynow+1.5\n",
        "        ymin=ynow-1.5\n",
        "        for t in self.total_user_distribution:\n",
        "          for i in t:\n",
        "            \n",
        "            if i[0]<=xmax and i[0]>=xmin and i[1]<=ymax and i[1]>=ymin:\n",
        "              coverd_users_now.append((i[0],i[1]))\n",
        "    \n",
        "    if (xnext,ynext) not in history:\n",
        "        \n",
        "        xmax=xnext+1.5\n",
        "        xmin=xnext-1.5\n",
        "        ymax=ynext+1.5\n",
        "        ymin=ynext-1.5\n",
        "        for t in self.total_user_distribution:\n",
        "          for i in t:\n",
        "            \n",
        "            if i[0]<=xmax and i[0]>=xmin and i[1]<=ymax and i[1]>=ymin:\n",
        "              coverd_users_next.append((i[0],i[1]))\n",
        "              \n",
        "              #print(i[0],i[1])\n",
        "    return len(list(set(coverd_users_next) - set(coverd_users_now)))  #when overlap only count the new people found\n",
        "\n",
        "\n",
        "  def step(self,state, action):\n",
        "        \n",
        "        self.battery+=-1\n",
        "        captured_ppl=0\n",
        "        # take one step\n",
        "        i, j = state[0]+3*action[0], state[1]+3*action[1]\n",
        "\n",
        "        # check if the next state is out off grid\n",
        "        if self.is_outoff_grid(i, j):\n",
        "            \n",
        "            #if it is in no fly zone  it will take its to  previous state\n",
        "            if i>=8 and i<= 14 and j>=10 and j<=13 :\n",
        "              i, j = state[0], state[1]\n",
        "            else :\n",
        "              i, j=np.abs(i), np.abs(j)   # if it is out of grid it will be refelcted\n",
        "              if i >20:\n",
        "                i=40-i\n",
        "              if j>20:\n",
        "                j=40-j\n",
        "\n",
        "          \n",
        "            \n",
        "            \n",
        "            reward=-5\n",
        "            if self.battery==0:\n",
        "              self.Done=True \n",
        "            self.state=(i,j)\n",
        "            return (i,j), reward,self.Done,captured_ppl\n",
        "\n",
        "        if i >=7 and i <= 9 and j>=18 and j<=20 :\n",
        "            reward = 20\n",
        "            self.Done=True\n",
        "            \n",
        "            self.state=(i,j)\n",
        "            return (i,j), reward,self.Done,captured_ppl\n",
        "        else:\n",
        "            captured_ppl=self.surveillance(self.total_user_distribution,state[0],state[1],i,j,self.history)  #current state and next state will be consdiered\n",
        "            reward =captured_ppl-1\n",
        "            if (i,j) in self.history:\n",
        "              reward=-10\n",
        "            if self.battery==0:\n",
        "              self.Done=True \n",
        "        self.state=(i,j)\n",
        "        return (i,j), reward,self.Done,captured_ppl\n",
        "\n",
        "  def reset(self):\n",
        "    #self.state=(0,0) #q1\n",
        "    Xint=np.random.choice([0, 1, 2])  #intial randomly selected [0,2]\n",
        "    Yint=np.random.choice([0, 1, 2])\n",
        "\n",
        "    self.state=(Xint,Yint)  #for Q2,Q3,Q4\n",
        "    self.battery=30\n",
        "    self.Done=False\n",
        "    self.history=[]\n",
        "    \n",
        "    return self.state "
      ],
      "metadata": {
        "id": "G_PJynC8RmhA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creat video\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "metadata": {
        "id": "6DHMv2yzSp_V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_policy_eval_video(policy,agent, filename,env, num_episodes=5, fps=3,):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      state = env.reset()\n",
        "      \n",
        "      done = False\n",
        "      while not done:\n",
        "        \n",
        "        action = actions[policy(np.array(state), agent)]                               \n",
        "        state,reward,done, _ = env.step(state,action)\n",
        "        \n",
        "        x=env.render('DDQN',state)\n",
        "        numpy_fig = mplfig_to_npimage(x)\n",
        "        video.append_data(numpy_fig)\n",
        "        \n",
        "  return embed_mp4(filename)"
      ],
      "metadata": {
        "id": "yiTh3W12SplA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "actions=([(0, 1), (0,-1), (-1,0), (1,0)])  #up,down,left,right\n",
        "#video=imageio.get_writer(\"final.mp4\", fps=3)\n",
        "\n",
        "#################################################################\n",
        "#Hyperparameters\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "\n",
        "#episode options\n",
        "max_episodes = 1000\n",
        "num_play_episodes = 10  # @param {type:\"integer\"}\n",
        "num_target_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval=20  # evaluation interval\n",
        "\n",
        "# learning options\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "gamma = 0.999 # was 0.99 # discount_factor\n",
        "epsilon = 1.0 # starting exploration rate\n",
        "epsilon_step = 10/max_episodes # epsilon step size\n",
        "min_epsilon = 0.05 # min exploration rate\n",
        "\n",
        "#optimizer options:\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "opt1 = \"tf.keras.optimizers.Adam(learning_rate = 0.001)\"\n",
        "#opt2 = \"tf.keras.optimizers.SGD(learning_rate = 0.001)\"\n",
        "\n",
        "\n",
        "# experience options\n",
        "num_fetch_experiences = 64 # number of experiences to fetch\n",
        "num_setup_experiences = int(num_fetch_experiences*2) # number of initial episodes to store was 100\n",
        "num_store_experiences = num_fetch_experiences\n",
        "exp_buffer_len = int(num_setup_experiences*2) # was 100000  \n"
      ],
      "metadata": {
        "id": "J0AaDUT8qUIs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################\n",
        "# DDQN AGENT\n",
        "\n",
        "class DDQN_Agent:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.train_Q = self._build_model() #Q Network\n",
        "        self.target_Q = self._build_model() #Target Network\n",
        "        \n",
        "    # copy weights of train_q network to target_q network\n",
        "    def update_target_Q(self):\n",
        "        self.target_Q.set_weights(self.train_Q.get_weights())\n",
        "        \n",
        "    # trains agent by calculating loss(TD error) following the present policy    \n",
        "    def train(self, batch, use_ddqn=True):\n",
        "        state_b, action_b, reward_b, new_state_b, done_b = batch #sampled from reply buffer\n",
        "        batch_len = np.array(state_b).shape[0]\n",
        "        state_b = tf.convert_to_tensor(state_b, dtype=tf.float32)\n",
        "        \n",
        "        # use training network to get Qvalues for a state\n",
        "        trainQ_b = self.train_Q(state_b)\n",
        "        targetQ_b = np.copy(trainQ_b)\n",
        "        \n",
        "        new_state_b = tf.convert_to_tensor(new_state_b, dtype=tf.float32)\n",
        "        \n",
        "        next_targetQ_b = self.target_Q(new_state_b)       \n",
        "        #used for ddqn\n",
        "        next_trainQ_b = self.train_Q(new_state_b)\n",
        "        # select max next state value\n",
        "        max_next_q_b = np.amax(next_targetQ_b, axis=1)\n",
        "        if (use_ddqn==True):\n",
        "            max_next_action_b = np.argmax(next_trainQ_b, axis=1)\n",
        "            for i in range(batch_len):\n",
        "                max_next_q_b[i] = next_targetQ_b[i][max_next_action_b[i]]       \n",
        "        # update target_q value with next state value from target network\n",
        "        for i in range(batch_len):\n",
        "            target_q_val = reward_b[i]\n",
        "            if not done_b[i]:\n",
        "                target_q_val += gamma * max_next_q_b[i]\n",
        "            targetQ_b[i][action_b[i]] = target_q_val\n",
        "        # fit the training network to match estimated reward for next state\n",
        "        train_hist = self.train_Q.fit(x=state_b, y=targetQ_b, verbose=0)\n",
        "        loss = train_hist.history['loss']\n",
        "        return loss\n",
        "    \n",
        "    # Define helper for hidden Dense layers with desired activation and init\n",
        "    def dense_layer(self, num_units):\n",
        "        return Dense(\n",
        "            num_units,\n",
        "            activation=tf.keras.activations.relu,\n",
        "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "                scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "    \n",
        "    # define helper function for output layer with desired activation and init\n",
        "    def q_values_layer(self, num_actions):\n",
        "        return Dense(\n",
        "            num_actions,\n",
        "            activation=None,\n",
        "            kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "                minval=-0.03, maxval=0.03),\n",
        "            bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "    \n",
        "    # build the model\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(self.dense_layer(100))\n",
        "        model.add(self.dense_layer(50))\n",
        "        model.add(self.q_values_layer(4))  #num of actions\n",
        "        \n",
        "        \n",
        "        print('ADAM')\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), loss='mse')\n",
        "        \n",
        "        #model.summary()\n",
        "        return model"
      ],
      "metadata": {
        "id": "q1ITqIixA326"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "\n",
        "# POLICIES\n",
        "\n",
        "''' My policy code here '''\n",
        "class Policy:\n",
        "    def __init__(self):\n",
        "        # self.epsilon = epsilon\n",
        "        self.epsilon=0.1\n",
        "    \n",
        "    # random policy independent of state\n",
        "    def random(self, state, agent):\n",
        "        # use random policy\n",
        "        action = np.random.choice([0, 1, 2, 3])  #gives index of the action\n",
        "        return action\n",
        "    \n",
        "    def update_epsilon(self, min_epsilon, epsilon_step):\n",
        "        if (self.epsilon > min_epsilon):\n",
        "            self.epsilon -= epsilon_step\n",
        "    \n",
        "    #epsilon greedy policy determines best action per state\n",
        "    def e_greedy(self, state, agent):\n",
        "        # use random policy epsilon percent of times\n",
        "        x = np.random.random()\n",
        "        if (x < self.epsilon):\n",
        "            action =  np.random.choice([0, 1, 2, 3])\n",
        "        else:\n",
        "            action = self.greedy(state, agent)\n",
        "        return action\n",
        "        \n",
        "    \n",
        "    # greedy policy selects best action\n",
        "    def greedy(self, state, agent):\n",
        "        # identify all possible actions for each state\n",
        "        state_tf = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
        "        #state_tf = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        state_Q = agent.train_Q(state_tf)\n",
        "        action = np.argmax(state_Q) # use in case multiple maximums\n",
        "        return action\n",
        "   "
      ],
      "metadata": {
        "id": "xBuawk_QBZIM"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "# DATA COLLECTION\n",
        "# REPLAY MEMORY BUFFER\n",
        "class Experience_Buffer:\n",
        "    \n",
        "    # initialize\n",
        "    def __init__(self):\n",
        "        self.exp_buffer = deque(maxlen=exp_buffer_len)\n",
        "        \n",
        "    # store experiences\n",
        "    def exp_store(self, environment, agent, policy, num_experiences):\n",
        "      \n",
        "      for i in range(num_experiences): \n",
        "          state=environment.state\n",
        "          action_index=policy.e_greedy(np.array(state), agent)  #modification\n",
        "          action = actions[action_index]\n",
        "                \n",
        "          new_state, reward, done, captured_ppl = environment.step(state, action)\n",
        "                \n",
        "          self.exp_buffer.append((state, action_index, reward, new_state, done))\n",
        "          if done:\n",
        "            environment.reset()\n",
        "             \n",
        "      #policy.update_epsilon(min_epsilon, epsilon_step)\n",
        "       \n",
        "      \n",
        "\n",
        "    # fetch experiences\n",
        "    def exp_fetch(self, num_experiences):\n",
        "        exp_batch = random.sample(self.exp_buffer, num_experiences)\n",
        "        state_b, action_b, reward_b, new_state_b, done_b = [],[],[],[],[]\n",
        "        for exp_sample in exp_batch:\n",
        "            state_b.append(exp_sample[0])\n",
        "            action_b.append(exp_sample[1])\n",
        "            reward_b.append(exp_sample[2])\n",
        "            new_state_b.append(exp_sample[3])\n",
        "            done_b.append(exp_sample[4])\n",
        "        return state_b, action_b, reward_b, new_state_b, done_b\n",
        "    "
      ],
      "metadata": {
        "id": "RjIeVMumEilV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#####################################################################\n",
        "# METRICS AND EVALUATION\n",
        "\n",
        "def calc_avg_return(environment, agent, policy, play_episodes):\n",
        "\n",
        "  total_return = 0.0\n",
        "\n",
        "  total_captured_ppl=0\n",
        "  all_optima_path=[]\n",
        "  temp_captured_ppl=[]    #used to identify the optimal captured among the episdoes\n",
        "  for i in range(play_episodes):\n",
        "\n",
        "    state = environment.reset()\n",
        "    episode_return = 0.0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    path = []\n",
        "    \n",
        "    \n",
        "    tem_comparision=0\n",
        "    \n",
        "\n",
        "\n",
        "    while not done:\n",
        "      action = actions[policy.greedy(np.array(state), agent)]\n",
        "      new_state, reward, done, captured_ppl = environment.step(state,action)\n",
        "      \n",
        "      total_captured_ppl+=captured_ppl\n",
        "      #print(\"is it done\",done)\n",
        "      path.append([state, new_state])\n",
        "\n",
        "      episode_return += reward\n",
        "      state = new_state\n",
        "      steps += 1\n",
        "\n",
        "      tem_comparision+=captured_ppl  #to see the optimal path\n",
        "\n",
        "    all_optima_path.append(path)\n",
        "    temp_captured_ppl.append(tem_comparision)\n",
        "\n",
        "\n",
        "    total_return += episode_return\n",
        "    #max_steps = max(max_steps, steps)\n",
        "    print('Game=',i,'Played steps=',steps,'Battery left',environment.battery)\n",
        "  \n",
        "  index_maximum_captured=np.argmax(np.array(temp_captured_ppl))\n",
        "  optimal_path=all_optima_path[index_maximum_captured]\n",
        "\n",
        "  avg_return = total_return / play_episodes\n",
        "  avg_ppl=total_captured_ppl/play_episodes\n",
        "\n",
        "  return avg_return,avg_ppl,optimal_path\n",
        "\n",
        "def plot_results(x,y,ylabel,title,eval_interval=1):\n",
        "    # Visualization: Plots\n",
        "    x_data = range(0, x,eval_interval)\n",
        "    \n",
        "    plt.plot(x_data, y)\n",
        "\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xlabel('episodes')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###### TRAIN THE AGENT #######\n",
        "def train_model(num_episodes,eval_interval):\n",
        "    env = enviornment()\n",
        "    agent = DDQN_Agent()\n",
        "    env.reset()\n",
        "    policy = Policy()\n",
        "    buffer = Experience_Buffer()\n",
        "    loss_b = np.zeros(num_episodes)\n",
        "    avg_rtn_b=[]\n",
        "    avg_ppl_total=[]\n",
        "    \n",
        "    #title of video\n",
        "    env.render()\n",
        "\n",
        "    filename='Trained_DDQN_Adam'\n",
        "    \n",
        "\n",
        "    # initialize the experience buffer with e-greedy policy\n",
        "    print('initialize the experience buffer with {:d} experiences\\n'.format(num_setup_experiences))\n",
        "    buffer.exp_store(env, agent, policy, num_setup_experiences)  #buffer holds= state,action_index, reward, new_state, done\n",
        "    \n",
        "    # begin running through episodes\n",
        "    print('begin running for {:d} episodes\\n'.format(num_episodes))\n",
        "    for episode in range(num_episodes):\n",
        "        # save a new set of experiences using e-greedy policy\n",
        "        buffer.exp_store(env, agent, policy, num_store_experiences)\n",
        "        # randomly select a batch of stored experiences\n",
        "        exp_batch = buffer.exp_fetch(num_fetch_experiences)\n",
        "        # retrain model with new batch of experiencesS\n",
        "        loss = agent.train(exp_batch)#############################\n",
        "        \n",
        "        # evaluate performance of the new model using greedy policy\n",
        "        if (episode+1) % eval_interval == 0:\n",
        "          avg_return, avg_ppl,optimal_path = calc_avg_return(env, agent, policy, num_play_episodes)\n",
        "\n",
        "          avg_rtn_b.append(np.asarray(avg_return))\n",
        "          avg_ppl_total.append(np.asarray(avg_ppl))\n",
        "          print('episode {:d} of {:d}, people captured: {:0.4f}, loss: {:0.4f}'.format(episode,max_episodes,avg_ppl,loss[0]))\n",
        "        # save loss and return\n",
        "        loss_b[episode] = np.asarray(loss)\n",
        "        \n",
        "        #max_steps_b[episode] = np.asarray(max_steps)\n",
        "        print('episode {:d} of {:d},loss: {:0.4f}'.format(episode,max_episodes,loss[0]))\n",
        "        if episode % num_target_episodes == 0:\n",
        "            print('\\tupdating target weights...')\n",
        "            agent.update_target_Q()\n",
        "    create_policy_eval_video(policy.greedy,agent, filename,env)\n",
        "    fig=env.render(optimal_path=optimal_path,path_plot=True)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    return (avg_rtn_b, loss_b,avg_ppl_total,optimal_path,fig)"
      ],
      "metadata": {
        "id": "nwK4Yi9BbUuZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# main \n",
        "\n",
        "''' train the model '''\n",
        "print('begin training the model!\\n')\n",
        "\n",
        "titleM = 'ddqn model'\n",
        "Return={}\n",
        "\n",
        "\n",
        "titleO = ', opt-Adam'\n",
        "print('running with Adam optimizer')\n",
        "title = titleM + titleO\n",
        "\n",
        "avg_returns, losses, avg_ppl_total,optimal_path,fig = train_model(max_episodes,eval_interval)\n",
        "Return['ddqn']=avg_returns\n",
        "print('done! Plotting results...')\n",
        "\n",
        "plot_results(max_episodes,avg_ppl_total,'Captured_People',title,eval_interval)\n",
        "plot_results(max_episodes,avg_returns,'average_returns',title,eval_interval)\n",
        "plot_results(max_episodes,losses,'losses',title)\n",
        "print('all done! Enjoy your chips!\\n')\n",
        "fig\n"
      ],
      "metadata": {
        "id": "ym0Bm-usrKPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}